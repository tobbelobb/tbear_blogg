<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="blogtemplate.xsl"?>
<posts year="2020">
  <post id="hangprinter_project_64" heading="hp-mark: Computer Vision for Hangprinter" date="11-11-2020">
    <figure>
      <a href="./bilder/hp-mark-logo.png">
        <img src="./bilder/hp-mark-logo.png" alt="" width="500" height="500"/>
      </a>
      <figcaption style="text-align:center">The hp-mark logo</figcaption>
    </figure>
    <p>
      As promised in the <a href="https://hangprinter.org/newsletter/#post_27">October Newsletter</a>,
      I've spent this month trying to add a computer vision system to Hangprinter.
      The system is called hp-mark, and is continuously published <a href="https://gitlab.com/tobben/hp-mark">here</a>.
    </p>
    <p>
      The core goal of hp-mark is to be able to measure the pose of the Hangprinter effector;
      its rotations and translations.
      Easy at it might sound, it's quite hard to get right.
      Going for developing such a big feature now has changed the direction of HP4's development in large.
    </p>
    <h3>Increment or Revolution?</h3>
    <p>
      The HP4 Prototype One, the finished and working machine that I mounted in my ceiling two years ago,
      is still just a prototype. No official version 4 of the Hangprinter has yet been released.
    </p>
    <figure>
      <a href="./bilder/HP4PT1_11_nov_2020_2.JPG">
        <img src="./bilder/HP4PT1_11_nov_2020_2_liten.JPG" alt="" width="500" height="498"/>
      </a>
      <figcaption>State of the HP4 PT1 hanging over me as I write this blog post.</figcaption>
    </figure>
    <p>
      The Prototype One can do some nice things, like
      <a href="https://www.youtube.com/watch?v=dEbyvdhdg88">printing OK Benchys</a>,
      and pushing out
      <a href="https://twitter.com/tobbelobb/status/1235466289377873920/photo/1">small pieces of furniture</a>.
      However, it's still just an incremental improvement over HP3.
      It has the same parts and features as HP3, just better.
      It also has the same fundamental limitations, just a tad less severe.
      It's still hard to calibrate, and lacks precision/accuracy guarantees, just like HP3 did.
    </p>
    <p>
      So I had a choice.
      I could settle on HP4 as an incremental improvement over HP3,
      or I could go for the big computer vision feature first.
    </p>
    <p>
      This is a classical problem of engineering management.
      The rule of thumb is to always do the incremental dance, because:
      <ul>
        <li>Wrong assumptions get surfaced faster. Saves eons of time and resources.</li>
        <li>You'll then always have an up-to-date working machine out there.</li>
      </ul>
    </p>
    <h3>Straying Away From Safety</h3>
    <p>
      I'm not going incremental, I'm making hp-mark as a from-scratch separate system.
      I'll try to face assumptions fast through structured testing in software.
      I'll also have to accept that my HP4PT1 machine won't be up-to-date or out there for a while.
    </p>
    <p>
      I've also strayed further away from the safe incremental track by deciding to use
      colored spheres as markers, instead of the standard flat ones:
    </p>
    <figure>
      <a href="./bilder/aruco.jpeg">
        <img src="./bilder/aruco.jpeg" alt="" style="border-radius:0px;display:block;margin:auto;"/>
      </a>
      <figcaption style="text-align:center">Standard flat fiducial marker.</figcaption>
    </figure>
    <p>
      There exist good technical arguments going spherical, but in honesty I discovered those after-the-fact.
      My hp-mark decision making process looked more like this:
    </p>
    <p>
      "I can not use the effector itself as a marker since I want to change it frequently.
      So, I must add markers.
      The markers must be pretty and practical.
      So, effector mountable spheres is my only option."
    </p>
    <figure>
      <a href="./bilder/synthetic_image_32mm_0_0_0_50_0_0_755.png">
        <img src="./bilder/synthetic_image_32mm_0_0_0_50_0_0_755_liten.png" alt="" width="500" height="262"/>
      </a>
      <figcaption>The currently preferred trade off between pretty and practical.</figcaption>
    </figure>
    <h3>Development Looks Promising</h3>
    <p>
      It has proven feasible to 3D-print small but accurate spheres (with some post processing):
    </p>
    <figure>
      <div class='embed-container'>
        <iframe src='https://www.youtube.com/embed/EFOeW-I1IgM'
          frameborder='0'
          allowfullscreen='true'>
        </iframe>
      </div>
    </figure>
    <figure>
      <a href="./bilder/mounted_sphere_marker_2.JPG">
        <img src="./bilder/mounted_sphere_marker_2_liten.JPG" alt="" width="500" height="281"/>
      </a>
      <figcaption>Not too far away from the render, eh?</figcaption>
    </figure>
    <p>
      Libraries like
      <a href="https://opencv.org/">OpenCV</a>
      and
      <a href="https://libcamera.org/">libcamera</a>
      work really well on the Raspberry Pi 4.
      Support, documentation and tools for Raspberry Pi 4 and the Picam in general
      has proven great.
      Tools like
      <a href="https://www.raspberrypi.org/documentation/usage/camera/raspicam/raspistill.md">raspistill</a>
      has given a lot of speed and convenience so far.
    </p>
    <p>
      I had to stitch together
      <a href="https://gitlab.com/tobben/hp-mark/-/tree/master/camera-calibration">a calibration program</a>
      from bits and pieces of OpenCV.
      It turned out robust enough to work with calibration patterns displayed directly on a flat-ish computer screen.
    </p>
    <figure>
      <a href="./bilder/charuco_on_monitor.JPG">
        <img src="./bilder/charuco_on_monitor_liten.JPG" alt="" width="500" height="281"/>
      </a>
      <figcaption>My monitor during camera calibration</figcaption>
    </figure>
    <figure>
      <a href="./bilder/monitor_curvature.png">
        <img src="./bilder/monitor_curvature_liten.png" alt="" width="500" height="312"/>
      </a>
      <figcaption>The calibration program is able to detect my monitor's 0.8mm curvature and compensate for it.</figcaption>
    </figure>
    <p>
      With this calibration, and very simple detector code, hp-mark can already find depth-positions
      shorter than 1500mm on real but simple test images with close to 1mm accuracy.
      My deck of benchmark images has only 7 real images, so take that 1mm number with a grain of salt for now,
      but it sure looks promising to me.
    </p>
    <figure>
      <a href="./bilder/sphere_detected.png">
        <img src="./bilder/sphere_detected_liten.png" alt="" width="500" height="376"/>
      </a>
      <figcaption>This image is taken only 233mm away from the marker. Click the image or zoom in to see the marker outline and center that hp-mark has drawn.
        The black border stems from undistorting (removing lens-effects from the Picam).
      </figcaption>
    </figure>
    <p>
      That's the same accuracy as my hand measurements,
      so I need a less manual way to generate my next deck of benchmark images.
      I've looked into using OpenScad for that purpose.
      It could generate benchmark images with known camera- and marker-locations.
    </p>
    <p>
      I didn't known the focal distance of the OpenScad camera, so I decided to generate calibration images as well:
    </p>
    <figure>
      <a href="./bilder/generated_calibration_image.png">
        <img src="./bilder/generated_calibration_image_liten.png" alt="" width="500" height="262"/>
      </a>
      <figcaption>Calibration image generated from within OpenScad. Click on image to get full size version.</figcaption>
    </figure>
    <p>
      If you look closely at the full size generated calibration image, you'll see that there are no grey pixels,
      only black/white ones, and that edges look chopped like a staircase.
      This made my camera calibration program regard the calibration pattern as non-flat:
    </p>
    <figure>
      <a href="./bilder/ridges.png">
        <img src="./bilder/ridges_liten.png" alt="" width="500" height="359"/>
      </a>
      <figcaption>"Flatness" of the calibration pattern, according to the camera calibration program
        after it had analyzed the calibration images generated from within OpenScad.
        Click on image to get larger version.
        Lines have been drawn in to emphasize the pattern of "ridges" and "valleys" that the calibration program
        found that it had to compensate out.</figcaption>
    </figure>
    <p>
      The script that generates the OpenScad calibration images and runs the camera calibration program on them,
      (ie how the "flatness" plot above was generated)
      can be found
      <a href="https://gitlab.com/tobben/hp-mark/-/blob/657e2ee303cad1d292a81f0b4804cb326b0899a2/camera-calibration/calibrate_openscad_camera/doit.sh">here</a>.
    </p>
    <p>
      I thought those ridges didn't look promising.
      Maybe the calibration program failed?
    </p>
    <p>
      To check if the calibration program had found the right values,
      I carefully deduced the OpenScad camera's focal length by hand.
      I managed to do that by generating a large series of images,
      and watch exactly when an object enters/disappears from the field of view.
      Details about how to do that are <a href="https://gitlab.com/tobben/hpm/-/blob/b2ab3c380823ba2f4e1d114ce7521c8b5a832f1c/hpm/example-cam-params/openscadHandCodedCamParams.xml">here</a>
      and
      <a href="https://gitlab.com/tobben/hp-mark/-/blob/4adcba5041ed8e07f098f257f7914ab137d32453/camera-calibration/calibrate_openscad_camera/test.scad">here</a>.
    </p>
    <p>
      With that very manual trick,
      I was able to confirm that the camera calibration program had found the correct focal distance,
      within a 0.5 px accuracy.
    </p>
    <p>
      Anyways, running hp-mark on the pretty and practical image above shows what we want to see:
    </p>
    <figure>
      <a href="./bilder/simpleBlobDetectorResult.png">
        <img src="./bilder/simpleBlobDetectorResult_liten.png" alt="" width="500" height="262"/>
      </a>
    </figure>
    <p>
      hp-mark prints out the following positions for the six spheres:
    </p>
    <code>
(139.999, 19.6511, 734.160)mm
( 74.4718, 93.1089, 823.895)mm
(-140.08, 19.6511, 734.160)mm
(-74.5623, 93.1093, 823.895)mm
(65.9931, -63.3267, 638.315)mm
(-66.0632, -63.3267, 638.315)mm
    </code>
    <p>
      I haven't yet verified that the positions are correct.
      All I've done is to be happy that they're so close to symmetrical.
    </p>
    <p>
      But now it's dinner. Nice to end the working day on a high note. Bye bye.
    </p>
  </post>

  <post id="moving" heading="This Blog Has Moved To torbjornludvigsen.com/blog" date="18-9-2020">
    <p>
      I'm entering a new period of full-time independent open source work.
      I figured that I will need a nice place to publish stuff.
    </p>
    <p>
      So, I've given my blog some love and moved it to a prettier domain =)=)=).
      I have made it easier to maintain, and also added some rudimentary analytics.
      (Publicly viewable, see <a href="https://torbjornludvigsen.goatcounter.com">https://torbjornludvigsen.goatcounter.com</a>.)
      The new page replaces the good old <a href="https://vitana.se/opr3d/tbear/index.html#Links">list of links</a> with more obvious and touch screen friendly navigation.
    </p>
    <p>
      This is the last post to be published on the old domain, vitana.se/opr3d/tbear.
    </p>
    <h3>Ehem. A Few Final Words</h3>
    <p>
      It's the end of a little mini-era.
      The blog started out in Jan 2014, as a hand-coded html document with 50 lines of hand-coded css.
      It wasn't even version controlled.
      A friend said "I can host your page", so we uploaded the html, the css, and the images to his server via command-line ftp.
      And it appeared on the web within 1 min, it was a bit magic to me.
    </p>
    <p>
      My mentality at the time was "keep everything extremely simple, and thresholds extremely low, and feel no prestige ever".
      So I kept the ftp deployment strategy, uploading each html and each image individually via command line.
      I also kept the long url, added no javascript, no analytics, no comments sections or anything.
      There was no platform around it, no ads, almost no readers.
    </p>
    <p>
      The extreme simplicity and low threshold worked.
      It got my productivity from 0 to something.
      I wrote simple posts about simple things.
    </p>
    <p>
      Web programming is very time consuming.
      The very simple <a href="https://torbjornludvigsen.com">torbjornludvigsen.com</a> web site took me 10 full working days
      to massage into its current shape.
      While my old ftp-based hand-coded system was clunky, it has also "just worked" for 6 years straight.
      Thanks sluggo setting it up and for hosting it.
      I hope the new page will be as simple and robust as what you set up for me in 15 minutes in 2014 :)
    </p>
  </post>
  <post id="personal_1" heading="Becoming a Type 3 Worker" date="25-8-2020">
    <p>
      I've been thinking about what I want to make this year.
      I've quit my job because I wanted to work on Hangprinter.
      No real plan. So now what?
    </p>
    <p>
      I think I will be heading where most of us will be heading.
      I think work is changing.
    </p>
    <h3>History of Worker Types</h3>
    <h4>Type 0</h4>
    <p>
      Let's skip hunter/gatherers.
      Let self-sufficient farmers be our basline type 0 workers.
      They spent most of their time producing food at home.
    </p>
    <figure>
      <a href="./bilder/Polish_berry_pickers_color.jpg">
        <img src="./bilder/Polish_berry_pickers_color_liten.jpg" alt="" width="500" height="354"/>
      </a>
      <figcaption>Mrs. Bissie and family in 1909. <a href="https://commons.wikimedia.org/wiki/File:Polish_berry_pickers_color.jpg">Image source</a></figcaption>
    </figure>
    <h4>Type 1</h4>
    <p>
      A few generations ago, work moved from farms into factories or factory-like workplaces.
      They had two pre-defined economical roles built-in, with often conflicting interests, packed tightly together: Workers and owners.
    </p>
    <p>
      Theres was some power struggle between them.
      Company employment structures were invented to regulate work relations, like families had done for type 0 workers.
    </p>
    <p>
      Type 1 workers got paid for working hours, so that they could buy stuff during non-work hours.
      They became type 1 worker/consumers.
    </p>
    <p>
      Work and consumption got separate physical spaces:
      Work places, and consumption places.
      Work and consumption also got separate time slots:
      9-5 work, and 5-9 consumption.
    </p>
    <p>
      This was the happy, but unsustainable equilibrium state worker/consumer type 1:
      Producing objects at work, buying objects at home, more and more, better and better.
    </p>
    <h4>Type 2</h4>
    <p>
      As factories moved abroad, work moved into office buildings.
      These produced no physical output.
    </p>
    <p>
      There was no new power struggle.
      Rather, there was a great consensus:
      Every single aspect of type 2 economic everyday life
      should mimic the type 1 system as closely as possible.
      Still more and more, better and better, only not objects.
    </p>
    <p>
      Workers got employed by companies.
      They were paid for being in offices, 9-5.
      They consumed 5-9.
      The worker and consumers roles were kept distinct.
    </p>
    <p>
      Words had to change meaning, to make type 1 and type 2 systems fit together.
      Words like "work", "consumption", and "employment" were all generalized.
    </p>
    <p>
      Work became "service".
      Consumption became "spending money".
      Employer became "shoehorn for legal formalities".
    </p>
    <figure>
      <a href="./bilder/Office_building.jpg">
        <img src="./bilder/Office_building_liten.jpg" alt="" width="500" height="333"/>
      </a>
      <figcaption>Image by: <a href="https://commons.wikimedia.org/wiki/User:XRay">Dietmar Rabich</a>, <a href="https://commons.wikimedia.org/wiki/File:Münster,_LVM,_Bürogebäude_--_2013_--_5149-51.jpg">Image source</a>, <a href="https://creativecommons.org/licenses/by-sa/4.0/legalcode" rel="license">CC BY-SA 4.0</a></figcaption>
    </figure>
    <p>
      A common way to get "employed" was to create a 1-person company, and simply hire onself.
      Most type 2 work was about feeding computers anyways.
      It not fit very well into the type 1 system.
    </p>
    <p>
      Some workers eventually stopped showing up at office.
      Instead, they fed computers while travelling.
      They called themselves "digital nomads".
      They were a hint about what was to come.
    </p>
    <h4>Early Type 3</h4>
    <p>
      Work soon moved out of offices and onto the Internet.
      The Internet was not factory-like or office-like at all.
      The consensus about mimicing the type 1 system broke down.
      There went 9-5/5-9 time slots, specific work buildings, and employers.
      Even monthly salaries went away.
    </p>
    <p>
      Type 3 workers didn't apply for their Internet work.
      They just created content, uploaded to Internet platforms, and waited for users.
      No formalities.
    </p>
    <p>
      Getting paid was more complicated and unpredictible.
      Rules and platforms were changing.
      Time, size, and even currency of payouts were in flux.
    </p>
    <figure>
      <a href="./bilder/disintegration-meme.png">
        <img src="./bilder/disintegration-meme_liten.png" alt="" width="500" height="277"/>
      </a>
      <!-- figcaption><a href="https://picsart.com/i/263056102004202">Image source</a></figcaption -->
    </figure>
    <h4>Late Type 3</h4>
    <p>
      So here we are, in a new era with less old structure, and with new built-in economic roles:
      Platforms, creators, and users.
      There is also new finite resource, to spend and earn: Attention.
    </p>
    <p>
      The meaning of the word "work" is moving towards "whatever someone pays attention to you doing".
      Attention now has a fairly predictible exchange rate into hard digital cash, via ads and donations.
    </p>
    <p>
      Like in the type 1 era, I think we will see power struggles.
      Platforms control the attention and write the contracts, without negotiations.
      For content creators it's like type 1 work before unions.
    </p>
    <p>
      Creators need to bond together, and gain the power to refuse bad deals.
      Together with users and platforms, they can create fair Internet rules.
      This will make platforms better.
    </p>
    <h4>My Work</h4>
    <p>
      I'm optimistic that platform work will be well regulated.
      The platforms are few and exposed.
      Creators and users are many and resourceful.
      If platforms misbehave too badly, users and creator unions can even make their own.
    </p>
    <p>
      I've started to think that a type 3 system that works well would be great.
      It would nurture the best creators, worldwide and on equal terms.
      It would give us more and more, better and better entertainment, science, inventions, ideas, thoughts.
    </p>
    <p>
      Maybe creators would get paid enough to become first class type 3 consumers, just like type 1 workers became type 1 consumers.
      Maybe, just maybe, our thoughts, packaged up like content, will get good enough to solve some of the problems we have in the world today.
    </p>
    <p>
      I think I will stop avoiding platforms.
      I'm not as scared of them anymore, even if they suck sometimes.
      I just have to learn how to use them.
    </p>
    <p>
      My new goal is to just keep publishing.
      Mostly RepRap.
      My new value proposition: Giving me money is like watering a flower.
    </p>
    <h4>By The Way</h4>
    <p>
      I've found a platform called Brave.
      It's a browser that reroutes ad- and content-based revenue streams toward themselves, creators, and users.
      It aggressively modifies websites before they're displayed, and has banking built-in.
      It might turn out to suck, like other platforms do,
      but it might also become what we need to turn attention into creator income via less ads.
    </p>
    <p>
      It has a list over content owners, and it wants to cover basically all content.
      I've registered as the owner of my content, so you can now support me for example like this:
    </p>
    <figure>
      <a href="./bilder/brave_button.png">
        <img src="./bilder/brave_button_liten.png" alt="" width="500" height="303"/>
      </a>
      <figcaption>Late type 3 work. You can barely see the human in there anymore. It sits behind the contribution buttons saying "thank you".</figcaption>
    </figure>
    <p>
      Also, if you get Brave Browser via <a href="https://brave.com/tor840">this link</a>, then I'll get a small kickback.
      So, hint hint, and thank you for reading.
    </p>
  </post>
  <post id="hangprinter_project_63" heading="Introducing Line Collision Detector" date="24-7-2020">
    <p>
      I got asked again recently, about the Hangprinter print volume.
      How big is it, and what shape is it?
    </p>
    <figure>
      <a href="./bilder/lincol_benchy.jpeg">
        <img src="./bilder/lincol_benchy_liten.jpeg" alt="" width="500" height="284"/>
      </a>
      <figcaption>
        A line collision. We don't want this.
      </figcaption>
    </figure>
    <h3>A Print Volume?</h3>
    <p>
      All 3D printers I know, except Hangprinter, have a <i>print volume</i>:
      A volume within which we can print freely,
      and outside which we can't print at all.
      A print volume has a shape and a size that doesn't change.
    </p>
    <p>
      The print volume is used to determine if an object is too big for a particular 3D printer.
      If the object doesn't fit in the printers print volume, then it is too big.
      Otherwise, it's not too big.
    </p>
    <h3>The Short Answer</h3>
    <p>
      How big is it? What shape is it?
      Sorry, can't answer that.
      A Hangprinter doesn't have a well defined print volume.
    </p>
    <p>
      I sometimes lie a bit and say "it's a big trumpet shaped print volume".
      That mostly gives a good enough intuition about what's printable with Hangprinter.
      And it saves me from having to formulate the long answer.
    </p>
    <h3>The Long Answer</h3>
    <p>
      Let's first restrict ourselves to objects that are printed layer by layer.
      Then, for any particular Hangprinter, assume that we could deduce a <i>max object</i>:
      An object that has a larger size than any other object that is printable with that Hangprinter.
      Let's call the volume enclosed by the max object <i>the max volume</i>.
      It would have <i>the max size</i> and <i>the max shape</i>.
    </p>
    <p>
      The max shape would probably look vaugely similar to a slightly triangluar trumpet.
      Finding it would be nice, and useful.
      The max size would roughly capture the size of the entire Hangprinter in one number.
      As with a print volume, we could for sure print freely within the max volume.
    </p>
    <p>
      However, Hangprinter would not be restricted to only ever print within the max volume,
      so the max volume would not be the print volume we're looking for.
      We'll explain this weird fact, but let's focus on two related, more practical questions first.
      <ul>
        <li>What positions are reachable?</li>
        <li>What objects are printable?</li>
      </ul>
    </p>
    <h3>The Reachable Volume</h3>
    <figure>
      <a href="./bilder/ReachableVolume.png">
        <img src="./bilder/ReachableVolume_liten.png" alt="" width="500" height="444"/>
      </a>
      <figcaption>
      </figcaption>
    </figure>
    <p>
      A hypothetical weightless Hangprinter effector would be able to reach any position within the tetrahedron spanned up by its four anchors.
      Let's call this <i>the enclosing volume</i>.
    </p>
    <p>
      Adding mass to the effector changes things slightly.
      Firstly, the machine then gains the ability to toss the effector out of this tetrahedonal envelope,
      so the volume is not an enclosing one anymore.
      Secondly, the machine looses the ability to keep the effector still near any of the tetrahedrons three upper faces.
      Mass will sag inwards towards the origin, no matter the stiffness of the lines or torque of the motors.
    </p>
    <p>
      Since Hangprinter can't print out an enclosing volume shaped object, the enclosing volume is not the print volume.
    </p>
    <p>
      Adding a sag to the enclosing volume gives us <i>the reachable volume</i>:
      The volume within which it's possible for the Hangprinter to position its effector and make it stay put.
      It would looks like this:
    </p>
    <figure>
      <a href="./bilder/hp_volume.png">
        <img src="./bilder/hp_volume_liten.png" alt="" width="500" height="293"/>
      </a>
      <figcaption>
        Exactly how much sag to expect can be calculated from the weight of the effector and the maximum static force of the motors.
        Fred Hedenberg made this nice rendering when investigating error due to line flex.
        Error due to limited motor power will have the same basic shape.
      </figcaption>
    </figure>
    <h3>Can We Print That?</h3>
    <p>
      We can't actually reach our whole reachable volume in a controlled way yet, since we don't have flex compensation in the firmware.
      But even ignoring flex, we still wouldn't have been able to print out an object with the shape and size of the reachable volume.
    </p>
    <p>
      The problem is, ABC lines point downwards, so they can collide with the half-finished print.
      All previously extruded material is potentially an obstacle for every following move.
      Every possible half-finished print state shadows out part of the reachable volume in its own unique way.
    </p>
    <p>
      Consider the following render for some intuition about line collisions:
    </p>
    <figure>
      <a href="./bilder/cone_shapes.jpg">
        <img src="./bilder/cone_shapes_liten.jpg" alt="" width="500" height="344"/>
      </a>
      <figcaption>
        The Hangprinter's effector is attached to the ground with six different lines.
        For each layer, each line will move within a "cone shape" (red).
        The top of each cone shape is a convex hull of the current top layer.
        Line collisions are marked in yellow.
      </figcaption>
    </figure>
    <p>
      Coming back to the max volume, we can now imagine how it's possible to print outside of it.
      We could add a wart on the max object and for every layer, make a render like the above.
      Then we could simply remove any part of the old max shape that turned yellow, in order to make the wart fully printable.
      Then if the wart turns yellow when any of the later layers are printed, we'll reshape those layers (by creating an inverted wart
      on the appropriate place) until all line collisions are avoided.
    <p>
    </p>
      That procedure would give us a large printable object that is not contained by the max volume.
      Hence we learn that different printable objects might take up different, mutually exclusive parts of the reachable volume.
    </p>
    <p>
      Ok, so the print volume doesn't exist, because of line collisions.
      Let's then revert to talking about <i>printable</i> versus <i>non-printable</i> objects.
      No more print volume, only reachable volume and printable/non-printable objects.
    </p>
    <h3>What Can We Print Then?</h3>
    <p>
      For every object that we want to print, we must perform a separate
      analysis to check whether a line collision would occur.
      The result of the analysis, depends on a lot of things, like:
    </p>
      <ul>
        <li>The positions of the anchors,</li>
        <li>the shape of the effector,</li>
        <li>how the object is rotated,</li>
        <li>where it's placed on the build plate,</li>
        <li>where we make our travel moves,</li>
        <li>and in which order we put down the material!</li>
      </ul>
    <p>
      Lots of stuff to think about, and for every single print.
      Sounds like we're in trouble?
    </p>
    <h3>The Solution</h3>
    <p>
      As complicated as the analysis sounds, it shouldn't have to be more than a small addition
      to the wealth of analyses that common slicer software already does for us before every single print.
      We as users should get a warning if a potential line collision is detected.
      The rest of the time, we shouldn't have to think about line collisions at all.
    </p>
    <p>
      Detecting line collisions isn't entirely trivial, but I'm happy to tell you that it's already done =D
      It's not baked into any slicer yet, but I've written a free-standing program who does the analysis separately.
      Let me present <a href="https://gitlab.com/hangprinter/line-collision-detector">line-collision-detector</a>.
    </p>
    <figure>
      <video width="500" height="341" controls="controls">
        <source src="bilder/usage00.mp4" type="video/mp4"/>
         <!--source src="movie.ogg" type="video/ogg"-->
         Your browser does not support the video tag.
      </video>
      <figcaption>
        The most basic usage of line-collision-detector.
        A collision is detected.
        The big-benchy.stl contains a 16.4x scaled up benchy.
        The params file contains positions of anchor points and effector pivot points.
        See the <a href="https://gitlab.com/hangprinter/linc/-/blob/f1282432a53a9c59042edb43716f4256b6de8caf/linc/params-example">params-example</a>
        file in the repo for information about the params file.
      </figcaption>
    </figure>
    <figure>
      <video width="500" height="340" controls="controls">
        <source src="bilder/usage01.mp4" type="video/mp4"/>
         <!--source src="movie.ogg" type="video/ogg"-->
         Your browser does not support the video tag.
      </video>
      <figcaption>
        Here, a non-scaled 3DBenchy is analyzed, and since it's so small, no collision is detected.
        The <code>-l</code> option is used to tell line-collision-detector to use a layer height of max 3 mm.
        A bigger l-value lets the program terminate faster because there are fewer layers to analyze.
      </figcaption>
    </figure>
    <figure>
      <video width="500" height="283" controls="controls">
        <source src="bilder/usage02.mp4" type="video/mp4"/>
         <!--source src="movie.ogg" type="video/ogg"-->
         Your browser does not support the video tag.
      </video>
      <figcaption>
        I recommend viewing this in fullscreen.
        Here, the <code>-o</code> option is used to create a debug model, which is inspected with <a href="https://www.blender.org/">Blender</a>.
        We can confirm that the effector is at a sensible position, and that there really is a line collision occuring at z=393.6.
      </figcaption>
    </figure>
    <p>
      For details about how to build and develop line-collision-detector, I refer to <a href="https://gitlab.com/hangprinter/line-collision-detector/-/blob/master/README.md">the readme in the line-collision-detector repo</a>
      and <a href="https://gitlab.com/hangprinter/linc/-/blob/master/README.md">the readme in the linc subrepo</a>.
      For details on how to use the program, I recommend typing <code>run --help</code> on the command line.
    </p>
    <p>
      This post is already quite long, so I realize I should save the details of the line-collision-detector
      algorithm for another blog post.
    </p>
    <p>
      Anyways, I hope you find this new tool useful!
      And well, I now kind of have a better short answer to "how big is the build volume?":
      Ca one 15.675x scale 3DBenchy =D
    </p>
  </post>
</posts>
